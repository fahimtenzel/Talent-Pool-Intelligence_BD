{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94624226-d6aa-4c8e-bc92-2e47b3aca733",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Silver Layer: Data Enrichment and Feature Engineering\n",
    "\n",
    "The **Silver Layer** takes the raw, validated data from the Bronze Layer (`bronze_resumes`) and transforms it into a structured, analysis-ready format. This layer performs **cleansing, standardization, and feature extraction** using PySpark UDFs to create new, valuable columns like 'extracted_skills'. The output is the `silver_features` Delta table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d30e1f5-f620-43db-aa51-3739970dad5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Imports required for UDFs and DataFrame operations\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "import re\n",
    "from pyspark.sql.types import ArrayType, StringType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "625668ce-2d28-47f7-b967-554aefb73e8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Feature Extraction: Defining the Skill Extractor (UDF)\n",
    "\n",
    "To analyze skill demand and cluster candidates, we must convert the raw text into structured features. This process defines a **User Defined Function (UDF)** in Python using Regular Expressions (`re`) to identify and extract key technical skills (Python, SQL, AWS, etc.) from the resume text. PySpark then applies this UDF to every resume in parallel across the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a51f5973-a331-4896-94c0-5f3303c011e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Skill extraction function and PySpark UDF registered.\n"
     ]
    }
   ],
   "source": [
    "# --- List of Common Data Science/Tech Skills ---\n",
    "SKILLS_LIST = [\n",
    "    r'\\bPython\\b', r'\\bSQL\\b', r'\\bSpark\\b', r'\\bDatabricks\\b', r'\\bR\\b', \n",
    "    r'\\bJava\\b', r'\\bC\\+\\+\\b', r'\\bC#\\b', r'\\bReact\\b', r'\\bAngular\\b', \n",
    "    r'\\bAWS\\b', r'\\bAzure\\b', r'\\bGCP\\b', r'\\bMachine Learning\\b', \n",
    "    r'\\bDeep Learning\\b', r'\\bTensorFlow\\b', r'\\bPyTorch\\b', r'\\bKeras\\b',\n",
    "    r'\\bTableau\\b', r'\\bPower BI\\b', r'\\bExcel\\b', r'\\bPandas\\b', r'\\bNumPy\\b',\n",
    "    r'\\bDocker\\b', r'\\bKubernetes\\b', r'\\bScikit-learn\\b', r'\\bScala\\b'\n",
    "]\n",
    "# Combine skills into a single, case-insensitive regex pattern\n",
    "SKILL_PATTERN = r'(?i)' + '|'.join(SKILLS_LIST) \n",
    "\n",
    "\n",
    "# --- Python Function to Extract Skills ---\n",
    "def extract_skills_from_text(text):\n",
    "  \"\"\"Uses regex to find and return a list of matching skills.\"\"\"\n",
    "  if text is None:\n",
    "    return []\n",
    "  \n",
    "  # Find all matches, clean, normalize, and remove duplicates\n",
    "  matches = re.findall(SKILL_PATTERN, text)\n",
    "  unique_skills = sorted(list(set([m.strip().lower() for m in matches])))\n",
    "  \n",
    "  return unique_skills\n",
    "\n",
    "\n",
    "# --- Register the Python function as a PySpark UDF ---\n",
    "extract_skills_udf = F.udf(extract_skills_from_text, ArrayType(StringType()))\n",
    "\n",
    "print(\"✅ Skill extraction function and PySpark UDF registered.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6dbcdb3e-acd3-4d55-85b3-5abfb552fcdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Silver Table Creation: Applying UDF and Saving\n",
    "\n",
    "This section reads the raw `bronze_resumes` table, applies the `extract_skills_udf` to create the new `extracted_skills` column, and saves the resulting feature-rich DataFrame as the `silver_features` Delta table. This table serves as the single source of truth for all subsequent machine learning models and analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77a3bbff-19bf-42b3-8d37-8ca0448ddce4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n✅ Created Silver DataFrame with extracted skills.\n+---------+----------------------+\n|Resume_ID|extracted_skills      |\n+---------+----------------------+\n|0        |[]                    |\n|1        |[python, sql, tableau]|\n|2        |[]                    |\n|3        |[]                    |\n|4        |[]                    |\n+---------+----------------------+\nonly showing top 5 rows\n\n\uD83D\uDCBE Silver table 'silver_features' created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Read the Bronze table created by the Bronze_Layer.ipynb notebook\n",
    "# This is the INPUT for the Silver Layer\n",
    "bronze_df = spark.table(\"bronze_resumes\")\n",
    "\n",
    "# Apply the UDF to the 'Resume_Text' column and create a new column 'extracted_skills'\n",
    "silver_df = bronze_df.withColumn(\n",
    "    \"extracted_skills\", \n",
    "    extract_skills_udf(F.col(\"Resume_Text\"))\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ Created Silver DataFrame with extracted skills.\")\n",
    "\n",
    "# Show the result to verify the extraction\n",
    "silver_df.select(\"Resume_ID\", \"extracted_skills\").show(5, truncate=False)\n",
    "\n",
    "# \uD83D\uDCBE Save the result as the SILVER Delta Table (OUTPUT of the Silver Layer)\n",
    "silver_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"silver_features\")\n",
    "\n",
    "print(\"\\n\uD83D\uDCBE Silver table 'silver_features' created successfully!\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Silver_Layer.ipynb",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}